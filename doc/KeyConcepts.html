
<html>
<head><title>Key Concepts</title>
<link href="style.css" rel="stylesheet" type="text/css">
</head>
<body>

<h1>Key Concepts</h1>

<p>
This section describes the major concepts employed by SilverKing and the corresponding meta
data constructs that support these concepts.
</p>

<a name="SilverKing Instances"></a>
<h2>Instances</h2>
<p>
This documentation will use the term instance to refer to either a distributed or
standalone instance of SilverKing. Each instance is identified by a name, a port,
and a ZooKeeper ensemble definition (all metadata is stored in ZooKeeper).
Typically, these three concepts are bound to a single Grid Configuration (described below).
Most SilverKing administrative commands work using Grid Configuration names.
By convention, SilverKing instance names begin with the prefix "SK.", though any
string is permitted. A typical name might be "SK.Reston.Example".
</p>

<p>
SilverKing instances are currently completely independent. Data stored in one
SilverKing instance is not visible in any other SilverKing instance.
SilverKing clients may, however, interact with multiple instances simultaneously.
</p>

<a name="Grid Configurations"></a>
<h2>Grid Configurations</h2>
<p>
A Grid Configuration is an abstraction used by SilverKing to simplify
administration by mapping a set of environment variables to a single identifier.
For SilverKing, grid configurations typically map the name, port, and ZooKeeper
ensemble definition of a SilverKing instance to a single identifier. 
Grid Configurations may also be used to map other variables used outside of
SilverKing (e.g. a grid computation framework) to the single identifier.
</p>

<p>
Grid Configurations can be named using arbitrary strings, but typicially
we use "GC_SK_" as the prefix of a grid configuration. For instance,
a SilverKing instance named "SK.Reston.Example" might have a Grid Configuration
named "GC_SK_RESTON_Example".
</p>

<a name="Namespaces"></a>
<h2>Namespaces</h2>
<p>
A namespace defines a logical namespace into which key, value pairs are stored.
Each namespace is associated with a storage policy.
</p>
<p>
Namespaces may be created in one of two ways: explicitly or automatically. 
Explicit namespace creation occurs when the createNamespace() function is called by a
SilverKing client. This method allows NamespaceOptions (discussed below) specific to this namespace to be specified.
Automatic namespace creation occurs when a namespace that has not been explicitly created
is first written to. In this case, the default NamespaceOptions for the given DHT will be
used. 
</p>

<a name="Versioning"></a>
<h2>Versioning</h2>

<p>
SilverKing provides rich support for data versioning. See the <a href="Versioning.html">Versioning</a>
for full details.
</p>

<a name="Topology"></a>
<h2>Topology</h2>

<p>
A topology defines a logical hierarchical relationship between entities such as servers, racks, datacenters, etc. 
A topology is typically constructed to reflect underlying physical constraints such as network latency, 
failure correlation, etc. A sketch of a typical topology is shown in <b>Figure 1</b>.
</p>

<table class="image" align="center">
<caption align="bottom"><b>Figure 1:</b> Topology</caption>
<tr><td><img src="TopologySketch.png" alt="Topology"/></td></tr>
</table>

<a name="Rings"></a>
<h2>Rings</h2>

<h3>Mapping</h3>
<p>
A fundamental function of SilverKing is mapping a user-supplied key to one or more SilverKing
daemons (replicas) that store the key. SilverKing performs this mapping using a "ring" structure.
This structure is realized using a range of the integers; each key to be stored in the ring is mapped to one of these integers as depicted in <b>Figure 2</b>. The mapping function used is a cryptographic hash
which ensures that keys map to unique integers (with extremely high probability).
</p>

<table class="image" align="center">
<caption align="bottom"><b>Figure 2:</b> Ring</caption>
<tr><td><img src="Ring.png" alt="Ring"/></td></tr>
</table>

<h3>Hierarchy</h3>

<p>
SilverKing rings are associated with nodes in a topology, and
each non-leaf element in a topology may have a ring associated with it.
This node in the topology is referred to as the ring's "owner".

<p>
The topological children of the owner are "members" of the ring. Each ring is divided into 
segments that are "owned" by ring members. The segment owners are responsible for storing all data in the segment.
For instance, a rack may have a ring associated with it where the servers in the 
rack are the ring members. 
</p>

<h3>Composition/Flattening</h3>

<p>
As all data is physically stored on servers, all rings with 
higher-level nodes in the topology must be composed with lower-level rings until a 
ring with only servers as members is obtained. This composed ring is actually used to store data.
</p>

<h3>Specification</h3>

<p>
A ring is specified as a topology, an owner (node instance) for the ring within the 
topology, an exclusion list, and two additional constructs: weights and storage policy.
</p>

<a name="Exclusion Sets"></a>
<h2>Exclusion Sets</h2>

<p>
In large SilverKing instances, hardware failure becomes inevitable. While it would be possible
to cope with hardware failures by introducing a new topology that excludes the failed server,
SilverKing provides a much simpler means to cope with failures: an exclusion set.
</p>

<p>
Exclusions define a list of servers that should not be included in any ring. 
Adding a server to the exclusion list precludes data from being stored on the server. This is typically 
done when the server is not functioning correctly or needs to be taken out of active use for some reason.
</p>

<p>
SilverKing uses two types of exclusion sets: a server exclusion sets, and instance
exclusion sets. 
</p>

<h3>Server Exclusion Sets</h3>
<p>
Server exclusion sets used to specify servers that should not be
used for any SilverKing instance. This is where servers with hardware-level issues should
be specified. This set is maintained outside of SilverKing typically as a flat file.
SilverKing monitors this file for changes, and update the server exclusion set whenever
a change is observed.
</p>

<h3>Instance Exclusion Sets</h3>
<p>
Each SilverKing instance also maintains an "instance exclusion set". This is a set
of daemons that are unhealthy (e.g. were killed, crashed, etc.).
</p>

<h2>Weights</h2>

<p>
A weight is a positive real number associated with each member of a ring. The total size 
of the segments owned by each member - and hence the members' share of the overall data - 
is proportional to their weights. Unless otherwise specified, the default weight for
servers is 1.0. For higher-level nodes in a topology, the default weight is the sum of the
weights of a node's descendants.
</p>

<a name="StoragePolicy"></a>
<h2>Storage Policy</h2>

<p>
A storage policy defines how data is stored within a ring. Currently, each value is stored in 
one or more members of the ring using replication. A storage policy specifies how each data 
item must be "stored" at each level of the ring (physical storage only takes place on servers.) 
Specifically, a storage policy specifies how many primary replicas and how many secondary replicas 
are used at the given level of the ring.
"Primary" replicas must store the data item. "Secondary" replicas will attempt to store data items, 
but are not required to.
In addition, storage policies allow the "binding" of specific members of the ring.
</p>
<p>
The following storage policy applies for data stored to a pod. It specifies that each data item 
must contain a primary replica in 1 rack and should contain a secondary replica in another rack. 
Within each rack, only a single replica is used.
</p>

<p>
<table class="storagePolicy" align="center">
<caption align="bottom"><b>Unbound Policy</caption>
<tr><td> 
<pre>
Pod:PodTestPolicyUnbound {
	primary {
		1 of Rack:RackTestPolicy
	}
	secondary {
		1 of Rack:RackTestPolicy
	}
}

Rack:RackTestPolicy {
	primary {
		1 of Server
	}
}
</pre>
</td></tr>
</table>
</p>

<p>
Like the previous example, the following policy applies to data stored in a pod.
This policy, however, requires 2 primary replicas to be stored - one in rack R1 and one in rack R2. 
In addition, a secondary replica should be stored in another rack. Within each rack, 1 replica is 
required and another is desired.
</p>
<p>
<table class="storagePolicy" align="center">
<caption align="bottom"><b>Bound Policy</caption>
<tr><td> 
<pre>
Pod:PodPolicyBound {
	primary {
		1 of Rack:RackTestPolicy:{R1},
		1 of Rack:RackTestPolicy:{R2}
	}
	secondary {
		1 of Rack:RackTestPolicy
	}
}

Rack:RackTestPolicy {
	primary {
		1 of Server
	}
	secondary {
		1 of Server
	}
}
</td></tr>
</table>
</pre>
</p>

<h3>Policy Resolution</h3>
<p>
As storage takes place only on servers, a storage policy is "resolved" to a server-level map of the ring.
After final resolution down to a server-level ring map, each ring segment is associated with a list 
of primary and secondary replicas. Using this map, it is possible to know exactly what primary and secondary servers should store any given key.

<a name="PassiveNodes"></a>
<h2>Passive Nodes</h2>
<p>
A "passive node" is a server that communicates with clients but does not actively store any data.
</p>

<a name="SilverKingInstance"></a>
<h2>SilverKing Instance</h2>

<p>
The previously discussed elements comprise all that is necessary to define a SilverKing DHT instance. 
Concretely, an instance is specified as a ring name, a ZooKeeper ensemble specification, and a passive node list name.
</p>

<a name="NamespaceOptions"></a>
<h2>NamespaceOptions</h2>

<p>
NamespaceOptions specify properties of created namespaces such as the StorageType and
ConsistencyProtocol to be used by a namespace. Explicitly namespace creation allows
users to specify options directly. Automatic creation will use the default NamespaceOptions
of a DHT.
</p>
<p>
Each DHT may specify whether it supports only explicit namespace creation, only automatic
namespace creation, or both. If both are supported, then a regular expression is specified.
This expression is then used to determine when to automatically create a namespace
for a put() operation that specifies a non-existent namespace.
</p>

<a name="RelationshipAmongElements"></a>
<h2>Relationship Among Elements</h2>

<p>
Each ring instance is associated with exactly one storage policy. Each storage policy may be associated 
with multiple ring instances.
</p>
<p>
Currently, each SilverKing DHT instance is only associated with a single ring, and - therefore - each DHT is also currently 
associated with only a single storage policy. Thus, all namespaces
within a DHT are associated with the same storage policy; this may change 
in the future.
</p>

<a name="MetadataMutation"></a>
<h2>Metadata Mutation</h2>

<p>
All metadata is versioned and never overwritten - we simply add a new version when we 
desired to make a change. For metadata that is derived from other base metadata (e.g. a ring), as base 
metadata changes, dependendent metadata is updated accordingly. For instance, as a topology or exclusion 
list changes, the ring mapping will be updated.
</p>

	
</body>
</html>
